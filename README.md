# Reinforcement_Learning

## PPO : Proximal Policy Optimization
이론 핵심: **PPO (Proximal Policy Optimization)**는 "학습을 하되, 너무 급격하게 성격을 바꾸지 마라"는 알고리즘입니다. 어제는 "오른쪽으로 가" 하다가 오늘 갑자기 "왼쪽으로 가" 하면 로봇이 혼란스러워하니까, 조금씩(Proximal) 정책을 수정해서 안정적으로 학습시킵니다. 그래서 로봇 제어 분야의 국룰(Standard)이 되었습니다.

## PPO의 두 개의 뇌 : Actor and Critic
### Actor (행동대장)

역할: Hopper의 센서값(기울기, 속도)을 보고 "다리를 펴라/접어라" 명령을 내립니다.

코드: net_arch=[256, 256]에서 앞부분에 해당합니다.

### Critic

역할: Actor가 움직이는 걸 보고 **"야, 지금 그 자세면 2초 뒤에 넘어져. 점수 낮아!"**라고 평가합니다.

중요성: 로봇은 넘어져봐야 아픈 줄 아는데, Critic이 미리 "그건 나쁜 자세야"라고 가치(Value)를 알려줘서 학습 속도를 높입니다.

> PPO의 핵심: Actor가 Critic의 조언을 듣고 행동을 고치는데, **"한 번에 너무 확 바꾸지 마!"**라고 제한을 겁니다.

## Principle of PPO(핵심 하이퍼파라미터 3개)
>Hopping Robot을 예시로

① clip_range (클리핑 범위): PPO의 안전벨트
PPO(Proximal Policy Optimization) 이름의 유래입니다.

상황: 로봇이 우연히 점프를 엄청 높게 해서 점수를 잘 받았습니다.

일반 RL: "우와! 무조건 세게 뛰어!" -> 다음 판에 너무 세게 뛰어서 뒤로 넘어짐. (학습 붕괴)

PPO: "점수 잘 받은 건 알겠는데, 원래 하던 스타일에서 딱 20%만 바꿔. 너무 흥분하지 마."

코드: clip_range=0.2 (기본값)

이게 PPO가 로봇 제어에서 짱 먹은 이유입니다. 학습이 튀는 걸 막아줍니다.

---

② ent_coef (엔트로피 계수): 호기심 조절기
로봇이 학습 초반에 자꾸 넘어지는 게 무서워서 가만히 서 있으려고만 할 때 건드리는 숫자입니다.

의미: "얼마나 엉뚱한 짓(탐험)을 허용할 것인가?"

낮으면(0.0): 하던 대로만 합니다. (안전빵, 고집불통)

높으면(0.1): 미친 척하고 다리를 막 찢어봅니다. (새로운 기술 발견 가능성 ↑, 하지만 잘 넘어짐)

실전 팁: 로봇이 너무 뻔한 움직임만 하면 이 숫자를 살짝(0.01) 올려줍니다.

---
③ learning_rate (학습률): 공부 속도
의미: 한 번의 경험에서 얼마나 많이 배울 것인가.

너무 크면: 성격이 급해서 이리저리 왔다 갔다 하다 망합니다.

너무 작으면: 전역할 때까지 걸음마도 못 뗍니다.

국룰: 3e-4 (0.0003)이 로봇 강화학습의 황금 비율입니다.

## Hopper의 성적표: 보상 함수 (Reward Function)
로봇이 "잘했다"고 칭찬받는 기준을 알아야 합니다. Hopper는 내부적으로 이렇게 점수를 받습니다.$$Reward = (앞으로 간 속도) - (모터 쓰는 힘 \times 0.001) + (안 넘어짐 \times 1.0)$$속도: 빨리 갈수록 점수가 팍팍 오릅니다.에너지 절약: 모터를 미친듯이 돌리면 감점입니다. (그래서 로봇이 나중에 보면 부드럽게 움직이려 노력합니다.)생존 보너스: 살아만 있어도 1점을 줍니다. (이게 없으면 로봇은 "움직이면 넘어지니까 그냥 바로 자살해서 게임 끝내야지"라고 생각할 수도 있습니다.)